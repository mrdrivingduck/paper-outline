# Outline

## EyeTell - Video-Assisted Touchscreen Keystroke Inference from Eye Movements - IEEE S&P 2018

Created by : Mr Dk.

2019 / 02 / 20 22:12

Nanjing, Jiangsu, China

---

### Introduction

密码推断攻击在移动设备中有许多攻击场景

在一个典型的场景中

受害者在一个不安全的公共环境下

使用自己的智能手机或平板的软键盘输入

攻击者推断受害者的输入（比如设备的密码）

基于推断出的密码

攻击者能够进行更多的攻击

很多密码推断依赖于分析受害者输入过程的录像：

* 分析触屏反射
* 手的动作
* 相关手指的移动
* 设备背部的运动

本文提出，通过一段记录了受害者眼部活动的视频

推断出受害者的密码

* 人们在软键盘上输入密码时，眼睛会聚焦并跟随输入的按键
* 因此输入一个密码序列会产生连续的眼部运动和独特的注视轨迹

三个主要的挑战：

1. 在没有任何先验知识的条件下，从视频中提取注视轨迹
   * 采用对用户独立的，基于模型的追踪方法
2. 注视轨迹噪声较多，很难恢复输入的序列
   * 开发了新的解码算法，对所有可能的输入序列进行评分
   * 输出评分最高的序列
3. 注视轨迹不能反映软键盘上精确的序列

本文的主要贡献：

* 提出了新的基于记录受害者眼部活动的视频进行的攻击
  * 攻击者不需要从视觉上看到受害者的解锁过程
  * 被攻击设备不需要被静态握持
* 原型在 Android 和 iOS 平台上进行了实验，包括：
  * PIN
  * 模式锁
  * 字母软键盘
* 指出了未来的改进方向和可能的对策

---

### Related Work

#### Keystroke Inference Attacks

##### Video-based Attacks

攻击者使用一个录好的视频推断密码

早期的攻击目标是物理键盘

* 利用附近物体上（玻璃、茶杯）的反射恢复电脑屏幕上的信息
* 通过视频中光的漫反射

需要攻击者直接录到受害者手指在物理键盘上输入的过程

最近的研究针对软键盘

* 通过受害者墨镜上的反射推断密码
* ...

需要攻击者录下的视频包含至少部分的输入过程或设备背面

相比之下，EyeTell 不需要与受害者设备或输入过程之间的无阻挡视线

只需要攻击者记录受害者在输入过程中的眼部运动即可

由于记录受害者的眼部活动更简单，也更隐蔽

因此这种攻击更实用

##### Sensor-based Attacks

* 移动设备的加速度传感器可被用于密码推断
* 加速度传感器和陀螺仪相结合
* 麦克风 + 前置摄像头

这些攻击需要攻击者从受害者设备获得传感器数据

* 要么通过恶意软件
* 要么通过未受保护的数据传输

这些假设在现实中未必能够满足

也有使用设备传感器作为侧信道来推断附近的物理键盘输入

* 通过分析附近一个恶意麦克风录下的声学信息，恢复物理键盘上的输入
* 通过分析声学信号的时间差推断输入
* 利用加速度传感器测量附近物理键盘输入产生的振动推断
* 利用受害者在输入时佩戴的手环中的加速度传感器推断

这些攻击假设攻击者能够从受害者设备中获得传感器的数据

或者其它设备的传感器能够在靠近物理键盘的位置采集数据

相比之下，EyeTell 能够在更远的距离下发动攻击

##### WiFi-based Attacks

攻击者通过 _Channel State Information, CSI_ 推断输入

* 不同的输入会导致无线信道中不同的变化，从而导致 CSI 的变化

已经证实，这种攻击对物理键盘和软键盘都有效

这些攻击和用户相关

需要攻击者提前获得带有标签的用户数据用于训练分类器

除此之外，这些攻击不能忍受环境中的任何变化

此外，在实验中：

* Wi-Fi 发送方和接收方的距离
* 受害者设备的方向
* 受害者的输入手势

都是固定的

这些局限使这些攻击无法在实际中使用

#### Eye-Tracking-Related Security Implications

##### User Authentication

利用眼部活动作为生物特征

##### Inferring User Input

受害者的眼睛会跟随触屏上的手指移动

这种特性会导致输入的泄露

EyeTell 有两点不同：

1. 之前的工作主要基于追踪较大的屏幕，而 EyeTell 追踪较小的移动设备屏幕
2. 之前的工作模式要求简单有效地追踪用户眼部活动，而 EyeTell 只能获得一个噪声较大的追踪，原因如下：
   1. 攻击者是在一定的距离之外录像的
   2. EyeTell 引入了一系列工具推断用户的输入

---

### Background on Video-based Gaze Tracking

注视 实际上指的是注视的方向

注视追踪 指的是决定眼睛注视方向的技术

目前，基于视频的注视追踪比较流行

* 不需要目标穿戴任何的设备就能获得较高的准确率

主要有两类基于视频的注视追踪方法：

* Feature-based
  * 使用本地特征，如轮廓、眼角，或眼部图像的反射
* Appearance-based
  * 直接使用眼部图像的内容作为输入

Feature-based 方法根据特征如何被使用，可被进一步划分为：

* Interpolation-based
  * 假设眼部图像特征和注视方向之间的映射可以被模型化为：
    * 参数形式，比如多项式
    * 非参数形式，比如神经网络
* Model-based
  * 直接通过图像特征，基于人眼的几何学模型，计算注视方向

在本文中，使用了 model-based 的追踪方法

* 攻击者不需要提前获得受害者的相关数据

---

### EyeTell Design

#### Video Recording

在受害者解锁过程中，通过录像记录受害者的眼部活动

假设触屏和受害者的手部活动都不能从视频中直接看到

#### Gaze Trace Extraction

采用用户独立的注视追踪方法

从每一帧中提取注视方向

将所有方向合并为一个完整的注视轨迹

在每一帧中分别检测两个眼睛

由于提取的注视轨迹存在噪声，且不稳定

进一步使用异常检测和低通滤波

以获得更干净的注视轨迹

#### Trace Decoding

设计了一个新的解码算法

将上一步中得到的注视轨迹

映射到软键盘上的一个候选序列集合中

对于单词或句子推断

还需要借助字典，选择有意义的结果

#### Word/Sentence Inference

通过字典查阅有含义的字母组合

选择可能的单词

---

### Summary

无心多读下去 后面的都看不懂了

---

